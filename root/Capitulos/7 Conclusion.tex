\chapter{Conclusiones} \label{ch:conclusion}

Este trabajo estudia la inferencia de BNN en dispositivos IoT de bajo consumo, utilizando como ejemplo un procesador RISC-V. Se han desarrollado herramientas de código abierto para transformar modelos BNN en punto flotante de TensorFlow a código C para inferencia utilizando solo precisión entera. Además, analiza diferentes métodos de optimización para el algoritmo de muestreo de pesos del algoritmo de inferencia, ya que consume la mayor parte del tiempo de ejecución.

Para acelerar la inferencia, este trabajo estudia el impacto en las métricas de incertidumbre y el coste de su implementación en software de una optimización propuesta en otro trabajo. Además este trabajo propone una nueva versión de dicha optimización que consiste en muestrear una distribución Uniforme. Esta optimización logra, en promedio, un \textit{speedup} de 5 en varios modelos representativos de BNN, conservando las métricas de incertidumbre en todos ellos.

Para mejorar estos resultados, se ha desarrollado una extensión para RISC-V que permite muestrear una distribución gaussiana usando solo una instrucción, implementada en un procesador de 32 bits. Las muestras se generan utilizando un GRNG basado en el TCL de bajo coste. La extensión acelera la inferencia hasta 8.10 veces, con reducciones similares en el consumo de energía y sin degradación significativa en la precisión o las métricas de incertidumbre. Se ha implementado el diseño en una FPGA Xilinx ZCU104. Su coste es solo de 240 LUTs y 320 Flip-Flops, generando un aumento del 0.65\% en el consumo de energía, sin afectar la frecuencia del reloj del sistema.

Como trabajo futuro se podría seguir iterando sobre la UF desarrollada permitiendo no solo generar muestras sino realizar toda la operación MAC requerida en la inferencia, lo que aumentaría aun mas el rendimiento. También se podría estudiar la portabilidad de la extensión desarrollada incluyéndola en otras CPU RISC-V abiertas. Por el lado del software, se podría añadir al motor desarrollado un marco de cuantización mas complejo, permitiendo reducir aun mas el tamaño de los modelos.
