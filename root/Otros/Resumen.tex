\begin{center}

\vspace{1cm}
{\Large \bfseries RESUMEN}

\vspace{2.5cm}
\end{center}

\boxtext{
\textbf{Paper abstract}\\
Neural Networks (NNs) are a very popular solution for classification tasks. As the combination of Internet of Things (IoT) with Machine Learning (ML), also known as TinyML, grows in popularity, more NN are being executed on low-end edge systems. The reliability of the predictions is crucial for safety-critical applications. Bayesian Neural Networks (BNNs) address this issue by calculating uncertainty metrics with their predictions at the cost of increasing computing requirements. This work addresses the challenges of executing BNNs inference on low-end systems. BNNs require multiple forward passes in which the weights are sampled from distributions. This sampling process can take up to 85,13\% of execution time. This work optimizes the weight sampling and integrates it within a low cost custom extension for a RISC-V CPU, improving speedup up to $\times$8,10 and similar energy savings.
}

\begin{itemize}
    \item RISC-V y extensiones
    \item Redes Bayesianas
    \item FPGA
\end{itemize}
